{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nguồn tham khảo: https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### các thư viện\n",
    "nltk: natural language toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aioz-\n",
      "[nltk_data]     interns/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import bz2\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "\n",
    "train_file = bz2.BZ2File('./amazonreviews/train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('./amazonreviews/test.ft.txt.bz2')\n",
    "\n",
    "train_file = train_file.readlines()\n",
    "test_file = test_file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "gồm 1000000 câu bình luận đi kèm với nhãn là tích cực (__label__2) và tiêu cực (__label__1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "num_train = 800000  # We're training on the first 800,000 reviews in the dataset\n",
    "num_test = 200000  # Using 200,000 reviews from test set\n",
    "\n",
    "train_file = [x.decode('utf-8') for x in train_file[:num_train]]\n",
    "test_file = [x.decode('utf-8') for x in test_file[:num_test]]\n",
    "\n",
    "print(len(train_file))\n",
    "print(len(test_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__1 super wack: just like No-Limit Cash Money has no shame at putting out garbage music.wack beats and no lyric ryhmes.who is buying this crab? all the stuff sounds the same and it's not that average.it's all bad.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print((train_file[300]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tiền xử lý data\n",
    "+ tách nhãn (0, 1) và câu ra riêng biệt\n",
    "+ thay các chữ số có trong câu thành số 0 (số có lẽ ko thể hiện sentiment)\n",
    "+ thay các url có trong câu thành chuỗi '< url >'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting labels from sentences\n",
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file]\n",
    "\n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]\n",
    "\n",
    "# Some simple cleaning of data\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
    "\n",
    "# Modify URLs to <url>\n",
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "super wack: just like no-limit cash money has no shame at putting out garbage music.wack beats and no lyric ryhmes.who is buying this crab? all the stuff sounds the same and it's not that average.it's all bad.\n",
      "0\n",
      "didn't run off of usb bus power: was hoping that this drive would run off of bus power, but it required the adapter to actually work. :( i sent it back.\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[300])\n",
    "print(train_sentences[300])\n",
    "print(test_labels[14])\n",
    "print(test_sentences[14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ lower case tất cả các word (từ 1 âm tiết)\n",
    "+ đếm số lần xuất hiện của chúng lưu vào words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done\n",
      "2.5% done\n",
      "5.0% done\n",
      "7.5% done\n",
      "10.0% done\n",
      "12.5% done\n",
      "15.0% done\n",
      "17.5% done\n",
      "20.0% done\n",
      "22.5% done\n",
      "25.0% done\n",
      "27.5% done\n",
      "30.0% done\n",
      "32.5% done\n",
      "35.0% done\n",
      "37.5% done\n",
      "40.0% done\n",
      "42.5% done\n",
      "45.0% done\n",
      "47.5% done\n",
      "50.0% done\n",
      "52.5% done\n",
      "55.0% done\n",
      "57.5% done\n",
      "60.0% done\n",
      "62.5% done\n",
      "65.0% done\n",
      "67.5% done\n",
      "70.0% done\n",
      "72.5% done\n",
      "75.0% done\n",
      "77.5% done\n",
      "80.0% done\n",
      "82.5% done\n",
      "85.0% done\n",
      "87.5% done\n",
      "90.0% done\n",
      "92.5% done\n",
      "95.0% done\n",
      "97.5% done\n",
      "100% done\n"
     ]
    }
   ],
   "source": [
    "words = Counter()  # Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
    "for i, sentence in enumerate(train_sentences):\n",
    "    # The sentences will be stored as a list of words/tokens\n",
    "    train_sentences[i] = []\n",
    "    for word in nltk.word_tokenize(sentence):  # Tokenizing the words\n",
    "        words.update([word.lower()])  # Converting all the words to lowercase\n",
    "        train_sentences[i].append(word)\n",
    "    if i%20000 == 0:\n",
    "        print(str((i*100)/num_train) + \"% done\")\n",
    "print(\"100% done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time \"we\" appear:  114499\n",
      "time \"suck\" appear:  2263\n"
     ]
    }
   ],
   "source": [
    "print('time \"we\" appear: ', words['we'])\n",
    "print('time \"suck\" appear: ', words['suck'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ xóa các từ chỉ xuất hiện 1 lần ra khỏi từ điển words (nhiều khả năng là vì sai chính tả)\n",
    "+ sort lại theo thứ tự xuất hiện (đồng thời xóa luôn số lần xuất hiện)\n",
    "+ thêm unknow và padding vào đầu từ điển\n",
    "+ tạo 2 dict để truy xuất vị trí của các word trong từ điển\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the words that only appear once\n",
    "words = {k:v for k,v in words.items() if v>1}\n",
    "# Sorting the words according to the number of appearances, with the most common word being first\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "# Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
    "words = ['_PAD','_UNK'] + words\n",
    "# Dictionaries to store the word to index mappings and vice versa\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "idx2word = {i:o for i,o in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suck\n",
      "top 20 words:  ['_PAD', '_UNK', '.', 'the', ',', 'i', 'and', 'a', 'to', 'it', 'of', 'this', 'is', ':', 'in', '!', 'for', 'that', 'was', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(words[word2idx['suck']])\n",
    "print('top 20 words: ', words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Encode các câu trong tập train và tập test bởi độ phổ biến của nó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(train_sentences):\n",
    "    # Looking up the mapping dictionary and assigning the index to the respective words\n",
    "    train_sentences[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n",
    "\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    # For test sentences, we have to tokenize the sentences as well\n",
    "    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else 0 for word in nltk.word_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66140, 88, 16, 3, 103103, 13, 11, 192, 459, 18, 361, 15, 9, 5736, 3, 91059, 14, 77, 433, 36, 90, 5, 51, 1662, 9, 88, 8, 141, 80, 616, 18520, 2, 211, 126, 15, 5, 27, 539, 3, 211, 18200, 2031, 22, 56, 10, 35, 10, 3, 792, 5, 27, 131, 539, 9, 58, 3, 96, 126, 15, 9, 7389, 261, 49, 4579, 62414, 6, 427, 7, 17664, 1077, 23, 8888, 2708, 6, 3932, 19348, 2, 9, 51, 4799, 204, 80, 2391, 8, 313, 15, 16999]\n",
      "[40, 99, 13, 28, 1445, 4274, 58, 31, 10, 3, 40, 1778, 10, 85, 1727, 2, 5, 27, 904, 8, 11, 99, 16, 152, 6, 5, 140, 89, 9, 2, 68, 5, 122, 14, 7, 42, 1845, 9, 210, 59, 243, 109, 2, 7, 134, 1845, 47, 29399, 38, 2640, 14, 3, 2378, 2, 11, 99, 47, 18877, 160, 2, 932, 30, 0, 0, 6, 557, 47, 1282, 2, 31, 10, 160, 21, 2334, 4156, 2, 11, 12, 7, 3564, 15134, 99, 14, 28, 24, 2, 182, 102, 130, 147, 9, 239, 12, 47, 821, 59, 2, 2582, 5, 262, 11, 4, 72, 598, 441, 4, 576, 4, 413, 4, 153, 4, 1686, 4, 1251, 1814, 519, 31, 179, 33, 80, 18, 17, 825, 62, 32]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[0])\n",
    "print(test_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### padding và shorten các câu để các câu có độ dài bằng nhau\n",
    "+ Các câu nào dài trên 200 thì bỏ phần sau, lấy 200 kí tự đầu\n",
    "+ Các câu nào ngắn hơn 200 thì thêm 0 (_PAD) vào đầu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
    "def pad_input(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "seq_len = 200  # The length that the sentences will be padded/shortened to\n",
    "\n",
    "train_sentences = pad_input(train_sentences, seq_len)\n",
    "test_sentences = pad_input(test_sentences, seq_len)\n",
    "\n",
    "# Converting our labels into numpy arrays\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Chia 20000 câu test ra 1 nữa để valid 1 nửa để test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.5 # 50% validation, 50% test\n",
    "split_id = int(split_frac * len(test_sentences))\n",
    "val_sentences, test_sentences = test_sentences[:split_id], test_sentences[split_id:]\n",
    "val_labels, test_labels = test_labels[:split_id], test_labels[split_id:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dùng 2 thư viện TensorDataset và DataLoader\n",
    "batch size: 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
    "val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_labels))\n",
    "test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
    "\n",
    "batch_size = 400\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = ('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "##### forward\n",
    "+ kích thước input ban đầu: batch_size x seq_len ( 400 x 200 )\n",
    "+ Tạo 1 bộ embedding vocab_size x embedding_size (vocab size: là số word, embed size: số thuộc tính của 1 word)\n",
    "+ Tiến hành embedding input -> thu được input đã được embedding: batch_size x seq_len x embedding_size ( 400 x 200 x 400)\n",
    "+ Đưa embedding input qua lstm (batch_first = True) cùng với 1 hidden state (được init zero) ban đầu (batch_size x ... x hidden_size) -> output (batch_size x output_size) cùng với hidden state (batch_size x ... x hidden_size )  \n",
    "+ Vì là số layer là 2 nên cần có 2 hidden layer được init ở dạng tuple\n",
    "+ lstm_out, hidden = lstm(inp, hidden), hidden ở đây là bộ hidden, nếu như có 1 layer thì nó cũng chính là lstm_out\n",
    "\n",
    "dấu ... = num_layers * num_direction (trong bài này là = 2x1) với num_layer là số hidden layer, direction là số chiều forward đối với input   \n",
    "\n",
    "\n",
    "contiguous trong đoạn code để đảm bảo phép view() reshape dữ liệu thành công: https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        # print(hidden[0].shape)\n",
    "        # print(lstm_out.shape)  torch.Size([400, 200, 512])\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        # print(out.shape) torch.Size([80000, 1])\n",
    "        out = out.view(batch_size, -1)\n",
    "        # print(out.shape)  torch.Size([400, 200])\n",
    "        out = out[:,-1]\n",
    "        # print(out.shape)  torch.Size([400])\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx) + 1  # số từ trong từ điển\n",
    "output_size = 1  # là tích cực hay tiêu cực (binary cross entropy)\n",
    "embedding_dim = 400 \n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "\n",
    "model = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "\n",
    "lr=0.005\n",
    "criterion = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train\n",
    "model.eval() vs torch.no_grad(): https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2... Step: 1000... Loss: 0.158818... Val Loss: 0.177383\n",
      "Validation loss decreased (inf --> 0.177383).  Saving model ...\n",
      "Epoch: 1/2... Step: 2000... Loss: 0.152957... Val Loss: 0.169877\n",
      "Validation loss decreased (0.177383 --> 0.169877).  Saving model ...\n",
      "Epoch: 2/2... Step: 3000... Loss: 0.180345... Val Loss: 0.167656\n",
      "Validation loss decreased (0.169877 --> 0.167656).  Saving model ...\n",
      "Epoch: 2/2... Step: 4000... Loss: 0.120877... Val Loss: 0.167090\n",
      "Validation loss decreased (0.167656 --> 0.167090).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(batch_size)\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip) # chống exploding gradient bằng cách nén mấy cái đạo hàm nhỏ lại\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.162\n",
      "Test accuracy: 93.962%\n"
     ]
    }
   ],
   "source": [
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('./state_dict.pt'))\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "h = model.init_hidden(batch_size)\n",
    "\n",
    "model.eval()\n",
    "for inputs, labels in test_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    output, h = model(inputs, h)\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
